# E3. Customer Data Toolkit {#sec-customer-data-toolkit-chapter .unnumbered}

## Why This Toolkit Exists

When you leave a customer conversation, open a survey, or ship a prototype, you’re not “collecting feedback”—you’re **running an experiment**. This toolkit keeps those experiments small, honest, and useful.

It’s not a research textbook. It’s a **bench manual**: what to do next, common traps, and the smallest move that gives you the next signal.

::: {.callout-note appearance="minimal" icon="tools"}
### How to Use This Toolkit

1. **Start with your urgent unknown.**  
   “What must we learn before we spend more money or time?”

2. **Pick the lightest method that fits.**  
   Conversation, survey, prototype, usage, pricing, observation, A/B.

3. **Name the trap → design the boost.**  
   Every section flags a failure mode and a fix.

4. **Run small, log outcomes, triangulate.**  
   Don’t bet big on a single signal—pair methods before you decide.
:::

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
### Ethics & Respect

- **Consent & privacy:** Be clear, ask permission, anonymize when possible.  
- **Licenses:** Use library and platform data within allowed terms.  
- **No dark patterns:** Evidence should reveal value, not trick people.
:::

> Rule of thumb: **Use the cheapest, fastest method to learn what matters—** and always ask whether the learning is worth the cost.


## 1. Conversations & Observation  
*Best for: exploratory learning when you don’t yet know the problem clearly.*  

### Methods  

- **Interviews** – One-on-one, open-ended conversations. Probe for stories and emotions, not just surface opinions.  
- **Focus groups** – Small group discussions that reveal shared patterns and group dynamics (but watch for conformity bias).  
- **Shadowing / ethnography** – Observe customers in their natural environment. Look for workarounds, shortcuts, and unspoken pain points.  
- **Customer journey mapping** – Track the steps customers take, noting touchpoints, frustrations, and hacks they invent.  

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
### Trap to Avoid  
Asking *“Would you buy this?”* too early. Early-stage interviews should uncover lived frustrations, not force a purchase decision.  
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
### Boost: Cross-Check Timeliness
Turn answers into **stories**.  
Instead of recording “They said checkout is slow,”  
capture: “She showed me her online cart — it had been sitting there for 3 weeks because the shipping fees felt too high.”  

Stories travel further in your mind (and your team’s mind) than bullet points.  
:::

---

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: Interview Structure  

A simple structure you can adapt:  

1. **Warm-up** – Easy questions to build rapport.  
   - “Tell me about the last time you [did X]…”  
2. **Stories & Pain Points** – Probe for specifics.  
   - “Walk me through what happened, step by step.”  
   - “What was the hardest part?”  
3. **Workarounds** – Learn how they cope.  
   - “What do you do instead when that happens?”  
4. **Dreams & Desires** – Open space for imagining.  
   - “If you could wave a magic wand, how would you want it to work?”  
5. **Wrap-up** – Invite connections.  
   - “Is there someone else you think I should talk to?”  
:::   

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: How to run a 1-hour observation
1. **Pick a setting** where your target customers naturally spend time (store, gym, café, online community).  
2. **Blend in / get permission.** Don’t make people feel like lab rats — be transparent if needed.  
3. **Watch workflows.** Note what people do step by step, where they get stuck, and what tools or workarounds they use.  
4. **Track artifacts.** What objects, devices, or hacks show up again and again?  
5. **Debrief fast.** Right after, jot down surprises, quotes, and hypotheses while fresh.

**Checklist — what to note:**  

- Where and when the activity happens  
- Tools, products, or workarounds in play  
- Interactions with others (help, bottlenecks, routines)  
- Gaps, frustrations, or “good enough” hacks  

**Ethics Reminder:** Respect privacy. Don’t record without permission. Step back if observing feels intrusive.
:::



### Quick Checklist  

- [ ] Aim for **stories, not opinions**.  
- [ ] Record (with permission) so you can listen more closely.  
- [ ] Ask “When was the last time you…?” not “Would you ever…?”  
- [ ] Thank participants — they’re helping shape your learning.  



## 2. Surveys & Questionnaires  

Surveys are the workhorse of customer data: fast, scalable, and structured. They help you move from *what you’ve heard* in a few conversations to *what’s common* across a broader group. But surveys are only as good as their design. A sloppy survey amplifies bias; a careful one gives you a clear signal.  

- **When to use**: After exploratory work, when you need to test hypotheses or measure how widespread a need or behavior is.  
- **Good for**: Quantitative validation, quick comparisons, segmentation.  
- **Not great for**: Deep discovery — if you don’t yet know the right questions to ask, go back to conversations.  

---

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
### Trap: Biased or Leading Questions  

Entrepreneurs often write questions that steer people toward the answer they want. For example:  

- “Wouldn’t you pay more for faster service?” (leading)  
- “Do you like or dislike our product?” (double-barreled: assumes use and asks two things at once)  

A biased survey can be worse than no survey — it gives you false confidence.  
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
### Boost: Structure Your Survey Like a Funnel  
The most reliable surveys flow like this:  

1. **Screeners** — Make sure respondents are in your target population.  
   - *Example*: “Have you purchased coffee in the last 30 days?”  
2. **Context/intro** — Briefly describe your product or problem.  
3. **Exploratory questions** — Open-ended first, to capture unprompted reactions.  
   - *Example*: “What do you like or dislike about current meal delivery services?”  
4. **Confirmatory questions** — Closed, structured, measurable.  
   - *Example*: “How often do you order meal delivery per week?”  
   - *Scale*: “On a scale of 1–5, how important is delivery speed to you?”  
5. **Wrap-up/demographics** — For segmentation (age, income, geography).  

This funnel keeps data clean and avoids priming or leading.  
:::

---

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: Writing Strong Survey Questions  

- Ask **one thing at a time**.  
- Keep wording neutral (“How satisfied are you?” not “How great is this?”).  
- Mix question types: open-ended for discovery, closed-ended for measurement.  
- Test your survey with 5–10 people first — refine before scaling.  
- Keep it short (under 10 minutes is ideal).  
:::

---

### Entrepreneur’s Use Case  

A subscription box startup suspected customers valued *convenience* more than *price*. They designed a survey:  

- Screened for recent subscription box buyers.  
- Asked open-ended questions about frustrations.  
- Measured importance of factors (price, convenience, variety, brand) on a 1–5 scale.  

Results showed 72% rated convenience “5 – very important,” while only 38% rated price as “5.” This insight shifted their positioning from “affordable” to “effortless.”  

---

### Quick Checklist  

- [ ] Define your audience and screen respondents.  
- [ ] Funnel from open → closed → demographic.  
- [ ] Avoid leading, double-barreled, and biased phrasing.  
- [ ] Pilot test before scaling.  
- [ ] Respect attention span (aim for 10 min or less).  

---


## 3. Prototypes & Tests  

Sometimes customers can’t tell you what they want — but they’ll *show you* when faced with something concrete. Prototypes and tests give you that feedback. They can be as simple as a sketch or landing page, or as advanced as an A/B test with real traffic.  

- **When to use**: After you’ve identified a need and want to see how people respond to a solution.  
- **Good for**: Measuring behavior, gauging demand, testing messaging.  
- **Not great for**: Broad discovery — prototypes work best once you have a hypothesis to test.  


::: {.callout-warning appearance="minimal" icon="alert-triangle"}
### Trap: Building Too Much, Too Soon  

Many founders over-invest in polished prototypes before testing core assumptions. A perfect app or product isn’t necessary. Even a “fake door” landing page (collecting clicks before the product exists) can answer your most urgent unknown.  
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
### Boost: Match Fidelity to the Question  
Choose the **lowest-effort prototype** that still answers your question:  

- **Sketch or storyboard** → “Do people even get what this idea is?”  
- **Clickable wireframe** → “Can people navigate this flow?”  
- **Landing page or fake ad** → “Will people sign up if offered this?”  
- **Concierge test** (manual service) → “Do customers actually use this when delivered?”  
- **A/B test** → “Which feature or message drives more engagement?”  
:::

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: Running a Landing Page Test  

1. Build a simple page with your value proposition and a sign-up button.  
2. Run small, targeted ads (e.g., Google, Facebook, LinkedIn) to your audience.  
3. Track **conversion rates**: how many click, sign up, or share.  
4. Compare variations (different headlines, images, or pricing) to see what resonates.  

Even 50–100 clicks can reveal whether interest exists.  
:::

---

### Entrepreneur’s Use Case  

A fintech startup wasn’t sure whether to emphasize “lower fees” or “faster transfers.” They ran two landing pages with identical designs but different headlines. After $200 in ad spend, the “faster transfers” page had 3x the sign-ups. That clarity saved them months of debate and development.  

---

### Quick Checklist  

- [ ] Start with the *lightest-weight* prototype that answers your question.  
- [ ] Make behavior measurable (clicks, sign-ups, conversions).  
- [ ] Keep experiments focused — test one key variable at a time.  
- [ ] Avoid over-polish; let evidence, not aesthetics, drive next steps.  
- [ ] Document learnings after each test — what did it prove or disprove?  



## 4. Usage & Analytics  
*“What do customers actually do once they’re in?”*

Once you have real users — even a small pilot group — usage data becomes gold.  
It tells you not what people *say* they’ll do, but what they *actually* do with your product or service.  


::: {.callout-warning appearance="minimal" icon="alert-triangle"}
**Trap:** Chasing **vanity metrics** (downloads, likes, page views).  
They look impressive, but they don’t show whether customers are finding value.  
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
**Boost:** Focus on **behavior tied to value**.  
Examples: retention, activation, repeat purchase, referrals.  
These tell you if customers are sticking, engaging, and paying.  
:::

::: {.callout-tip appearance="minimal" icon="lightbulb"}
**Mini-Case — From Downloads to Retention**  

A student team launched an app that quickly hit **5,000 downloads**.  
They celebrated — until usage data showed that **80% of users never opened the app again after day one**.  

Instead of chasing more downloads, they built a **simple funnel**:  

- Stage 1: Download  
- Stage 2: First login  
- Stage 3: Second-week active  

The funnel revealed the **biggest drop-off was at login** — onboarding was confusing.  
After fixing the sign-up flow, their second-week active rate tripled, even though total downloads grew more slowly.  

**Lesson:** Better to have **100 loyal users** than 5,000 ghosts.  
:::

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide — Build a Simple Funnel

1. **Define stages**: Awareness → Sign-up → First use → Repeat use → Referral/Upgrade.  
2. **Instrument touchpoints**: Track actions that mark progress (account created, first order placed, second order placed).  
3. **Calculate conversion at each stage**: % who move from one step to the next.  
4. **Spot the drop-offs**: Where do most people exit? That’s your design or value gap.  
5. **Run one small change** (better onboarding, clearer CTA) → measure again.  

Even a spreadsheet with 20 users can show you the shape of your funnel.  
:::

---

### Entrepreneur’s Checklist — Usage & Analytics  

- [ ] Define your **north star metric** (the best single proxy for delivered value).  
- [ ] Identify **3–5 supporting metrics** (activation, retention, referrals, revenue per user).  
- [ ] Avoid **vanity metrics** — only track what ties to value.  
- [ ] Instrument **early** (don’t wait until launch).  
- [ ] Review usage data weekly, pair it with customer conversations.  
- [ ] Log changes and re-measure (so you can tell cause from coincidence).  



## 5. Pricing & WTP (Willingness to Pay)

Pricing is one of the most direct ways to test whether a customer values what you’re building.  It is one of the sharpest evidence levers in your business model — but also one of the trickiest to set.  
<!-- But beware: **asking “what would you pay?” is almost always useless.**  
People guess, posture, or give socially acceptable answers. To get meaningful signals, you need structured methods. 

The most common approaches to pricing use survey methods such as Gabor-Granger, Van Westendorp, and conjoint analysis.  Behavioral test such as A/B testing are even stronger because they leverage actual customer buying behavior.  -->

---

### Common Approaches

#### 1. Direct Surveys *(weakest)*
- Simply asking: “What would you pay?”  
- Problem: answers drift high or low, disconnected from real choices.  
- Use only for **exploratory stage** (hypothesis fuel), not decisions.

#### 2. Gabor-Granger
- Ask: “Would you buy at $X?” → Yes/No.  
- If yes, increase; if no, decrease.  
- Produces a **distribution of acceptance** across price points.

::: {.callout-tip appearance="minimal" icon="lightbulb"}
### Mini-Case — SaaS Price Banding  
A startup ran a Gabor-Granger survey testing \$10, \$25, \$50. Interest was flat below \$20, collapsed above \$40.  
They couldn’t see the exact optimum, but they learned their *“live zone” was $25–35*. That gave confidence to launch, while keeping room to optimize later.  
:::

#### 3. Van Westendorp (Price Sensitivity Meter)
- Four questions:  
  - Too cheap?  
  - Cheap/good value?  
  - Expensive but worth considering?  
  - Too expensive?  
- Overlapping responses give a **range** of acceptable prices.  
- Best for quick reads with moderate sample sizes.

::: {.callout-tip appearance="minimal" icon="lightbulb"}
### Mini-Case — The Meal Kit Price Range

A meal kit startup surveyed potential customers with Van Westendorp.  
They found:  

- Below $40/week → “too cheap, must be low quality.”  
- $50–70/week → “good value, worth trying.”  
- Above $90/week → “too expensive.”  

Instead of one “ideal price,” they learned they had a **band**:  

- $50/week entry offer (trial, acquisition).  
- $70/week core plan (retention sweet spot).  

Decision: position offers as tiers within the acceptable range.
:::

#### 4. Conjoint / Choice-Based Analysis
- Present bundles of features *with prices*.  
- Customers choose between options.  
- Reveals **tradeoffs**: which features justify higher prices, which don’t.  
- More complex, but very powerful.

#### 5. Behavioral Tests *(strongest)*
- Actual transactions: pilots, pre-orders, A/B pricing.  
- Incentive-compatible: customers put real money (or time) at stake.  
- Most credible but also riskiest.

---

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
**Trap:** Taking stated prices at face value.  
Customers often say they’ll pay more than they actually will. Always validate with behavior if possible.
:::

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
**Trap:** All of these methods are **heuristics**. They help you avoid being wildly wrong — but they can’t tell you the “right” price.  

- Van Westendorp → *acceptable window*  
- Gabor-Granger → *stated intent curve*  
- Conjoint → *relative value of features*  
- A/B test → *winner between two points*  

None produces a true **demand curve** linked to actual purchasing behavior, which is what you need to calculate profit-maximizing price.
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
**Boost:** Frame tradeoffs.  
Instead of “what would you pay?”, ask customers to choose between **Option A: $20 / basic features** and **Option B: $30 / premium features**.  
Choices reveal value better than single numbers.
:::

> **Lesson:** WTP is not a single point — it’s a spectrum. Smart founders design pricing **within** that band.

### Beyond Surveys: The Economic Method  
If you actually want the “right price,” you need to estimate a **demand curve** from WTP data and link it to your costs.  
That’s where profit analytics comes in — turning customer data into full demand and profit models. It’s beyond this chapter, but you’ll see it in detail in the profit analytics framework in [*Hatch It or Hatchet*](https://ea.nilehatch.com).

---

### Quick Guide: Which Method to Use?

- **Early stage (explore):** Direct survey or Van Westendorp.  
- **Feature tradeoffs:** Conjoint.  
- **Quick pricing bands:** Gabor-Granger.  
- **Validation:** Pre-orders, pilots, or live A/B tests.
- **Setting price:** Profit analytics using willingness to pay data

### Checklist: Using WTP Wisely

1. [ ] **Treat surveys as directional** → Use them to set boundaries and identify safe zones, not as calculators.  
2. [ ] **Cross-check with behavior** → Pair with pre-orders, pilots, or transaction data whenever possible.  
3. [ ] **Think in bands, not points** → Price within a “likely zone” rather than chasing false precision.  
4. [ ] **Translate to decisions** → The goal is to eliminate bad options and focus your next test.  



**Rule of thumb:** the closer you get to real money, the more you can trust the signal.

**Bottom Line:** Survey-based WTP methods are like a flashlight in a dark room. They help you see where not to step, but they don’t show you the summit. For true pricing strategy, you’ll need demand modeling — the economist’s tool for finding the price that maximizes profit.


## 6. A/B and Live Experiments  

Sometimes the only way to know what works is to **put two options in the wild** and see which performs better.  
That’s the logic of **A/B testing**: show version A to some people, version B to others, and measure the outcome.  

Unlike surveys or interviews, A/B tests capture **real behavior** in context — clicks, signups, purchases.  
That makes them one of the strongest ways to test messaging, features, and flows.  

---

### When to Use
- You’ve narrowed down to a few options and want evidence on **which one converts better**.  
- You have enough traffic, users, or participants to run a fair comparison.  
- You’re deciding between **small but critical choices** (pricing page layout, CTA wording, email subject lines).  

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
**Trap to Avoid:** Testing everything at once.
If you change the headline, button color, and offer simultaneously, you’ll never know what drove the difference.  
Keep it simple: change **one variable at a time**.  
:::

::: {.callout-note appearance="minimal" icon="check-circle"}
**Boost:** Start with *low-cost, high-leverage tests*:  

- Which landing page headline gets more signups?  
- Which subject line drives higher open rates?  
- Which of two onboarding flows reduces drop-off?  

Even micro-wins add up — a slightly better conversion rate compounds through the funnel.  
:::

::: {.callout-tip appearance="minimal" icon="lightbulb"}
#### Mini-case
A subscription app tested two signup flows:  

- **Flow A:** 7-day free trial → paywall.  
- **Flow B:** Paywall upfront with a money-back guarantee.  

Flow A had higher signups, but Flow B had higher **paid conversions**.  
The team chose Flow B — fewer users, but more revenue.  

**Lesson:** Define the right success metric before testing.  
Clicks feel good, but **revenue wins**.  
:::

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: Run Your First A/B Test
1. **Pick one variable.** Headline, CTA, or layout element.  
2. **Split randomly.** Show A to half your traffic, B to the other half.  
3. **Measure a clear outcome.** Click-through rate, signup rate, purchase completion.  
4. **Run until stable.** Don’t peek too soon — small samples create noise.  
5. **Decide.** Keep the winner or test again.  
:::

---

### Checklist
- [ ] **Randomization:** Each user should have equal odds of seeing A or B.  
- [ ] **Sample size:** Too few visitors = false signals. (As a rule of thumb: wait for at least 100+ conversions per arm before calling it.)  
- [ ] **Significance:** Look for consistent differences, not just one lucky spike.  
- [ ] **Practicality:** The better variant should meaningfully improve your metric, not just by 0.2%.  



## 7. Triangulation & Iteration  

No single method gives you the whole truth.  
Interviews tell you what people **say**.  
Surveys hint at what people **think**.  
A/B tests show what they actually **do**.  
The magic is in **triangulation** — layering methods to see where signals converge.  

---

### When to Use
Always. Every method has blind spots. Triangulation lets you **separate noise from signal** and move forward with confidence.  

---

::: {.callout-warning appearance="minimal" icon="alert-triangle"}
### Trap to Avoid
Overweighting one method. Just surveys. Just interviews. Just click data. Each one alone can fool you.  
:::

::: {.callout-note appearance="minimal" icon="alert-triangle"}
### Boost
Pair across types:  

- **Qualitative + Quantitative**: Interview themes, then measure at scale.  
- **Stated + Behavioral**: Ask what they prefer, then see what they actually choose.  
- **Fast + Deep**: Quick signals for speed, rich methods for context.  
:::

::: {.callout-important appearance="minimal" icon="clipboard"}
### Mini-Guide: Triangulating an Insight
1. **Interview (Qualitative):** 10 early users say onboarding feels “confusing.”  
2. **Survey (Quantitative):** 200 users → 42% rate onboarding <3 stars.  
3. **Usage Data (Behavioral):** Analytics show 55% drop-off after step 2.  

**Decision:** Redesign step 2, not the whole flow.  
**Lesson:** Triangulation narrowed a vague complaint into a precise fix.  
:::

---

### Checklist
- [ ] Record assumptions before testing.  
- [ ] Log confidence levels after each signal.  
- [ ] Cross-check with a second method before acting big.  
- [ ] Use iteration: one method sparks the next round of questions.  



## Entrepreneur’s Master Checklist  
::: {.callout-tip appearance="minimal" icon="check-circle"}
1. **Frame the urgent unknown**  
   → What question must you answer now?  

2. **Pick the matching method**  
   → Conversation, survey, prototype, usage, pricing, observation, A/B.  

3. **Spot the trap**  
   → Every method has one — design in the boost.  

4. **Run light. Learn fast. Log results.**  
   → Keep experiments small but cumulative.  

5. **Triangulate before you decide**  
   → Don’t move big on a single signal.  
:::


This toolkit of methods is your lab bench.  
Run the right experiment, at the right time, in the right way.  
Then — when the evidence lines up — make the call.  


